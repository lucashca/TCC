import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.isotonic import IsotonicRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn import metrics

#csv = pd.read_csv("novdataset.csv",usecols=[1,2,3,4,5,6,7])
csv = pd.read_csv("sela.csv",usecols=[0,1,2])


def fixCsv(arr):
    #Corrige o dataset, convertendo strig para float e tambÃ©m coverte a unidade dos nutriente de mg/l para cmolc/dm3
    for data in arr:
        data[0] = float(data[0])
        data[1] = float(data[1])
        data[2] = round(float(data[2])/200.4,2) #Ca
        data[3] = round(float(data[3])/121.56,2) #Mg
        data[4] = round(float(data[4])/230,2) #Na
        data[5] = round(float(data[5])/391,2) #K
        data[6] = round(float(data[6])/506.47,2) #Cl
        
    return arr


def getMaxValues(data):
    maxValues = []
    for i in range(data.shape[1]):
        maxValues.append(max(abs(data[:,i])))
    return maxValues

def getMinValues(data):
    minValues = []
    for i in range(data.shape[1]):
        minValues.append(min(abs(data[:,i])))
    return minValues

def getMedias(data):
    medias = []
    for i in range(data.shape[1]):
        medias.append(sum(data[:,i])/data.shape[0])
    return medias

def getMaiorMedia(data,media):
    maiorMedia = []
    for i in range(data.shape[1]):
        m = []
        for d in data[:,i]:
            if(d > media[i]):
                m.append(d)
        maiorMedia.append(m)
    return maiorMedia

def getMenorMedia(data,media):
    maiorMedia = []
    for i in range(data.shape[1]):
        m = []
        for d in data[:,i]:
            if(d < media[i]):
                m.append(d)
        maiorMedia.append(m)
    return maiorMedia

def normalizeData(dataSet):
    maximos = getMaxValues(dataSet)
    minimos = getMinValues(dataSet)
    data = np.zeros(dataSet.shape)
    
    for i in range(dataSet.shape[1]):
         data[:,i] = (abs(dataSet[:,i]) - minimos[i])/(maximos[i] - minimos[i])
        #data[:,i] = abs(dataSet[:,i]/maximos[i])
    return data





#dataSet = np.array(fixCsv(csv.values))
dataSet = np.array(csv.values)

maxValues = getMaxValues(dataSet)
medias = getMedias(dataSet)
maiorMedia = getMaiorMedia(dataSet,medias)
menorMedia = getMenorMedia(dataSet,medias)


data = normalizeData(dataSet)

#dataFrameOriginal = pd.DataFrame({"Latitude":dataSet[:,0],"Longitude":dataSet[:,1],"Ca":dataSet[:,2],"Mg":dataSet[:,3],"Na":dataSet[:,4],"K":dataSet[:,5],"Cl":dataSet[:,6]})
#dataFrameNormalizado = pd.DataFrame({"Latitude":data[:,0],"Longitude":data[:,1],"Ca":data[:,2],"Mg":data[:,3],"Na":data[:,4],"K":data[:,5],"Cl":data[:,6]})

dataColun = 2
xTrain, xTest, yTrain, yTest = train_test_split(data[:,0:2], data[:,dataColun], test_size = 0.25, random_state = 0)


regression = [
    ["SVR - RBF", SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)],
    ["SVR - Linear",SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)],
    ["SVR - Poly",SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,coef0=1)],
    ["Linear Model",linear_model.LinearRegression()  ],
    ["Random Forrest Regressor",RandomForestRegressor(max_depth=2, random_state=0,n_estimators=100)],
    ["Decision Tree Regression 1", DecisionTreeRegressor(max_depth=1)],
    ["Mult Layer Perceptron",MLPRegressor(solver='lbfgs', learning_rate ="adaptive",alpha=1,hidden_layer_sizes=(100), random_state=1)],
    ["Ridge",linear_model.Ridge(alpha=.5)],
    ["RidgeCV",linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))],
    ["Lasso",linear_model.Lasso(alpha=0.1)],
    ["ElasticNet",linear_model.ElasticNet(alpha=0.1)],
    ["LassoLars",linear_model.LassoLars(alpha=0.1)],
    ["OrthogonalMatchingPursuit",linear_model.OrthogonalMatchingPursuit()],
    #["Logistic Regression", linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')]
    ["SGDRegressor",linear_model.SGDRegressor()],
    ["BayesianRidge",linear_model.BayesianRidge()],
    ["ARDRegression",linear_model.ARDRegression()],
    ["PassiveAggressiveRegressor",linear_model.PassiveAggressiveRegressor()],
    ["TheilSenRegressor",linear_model.TheilSenRegressor()],
     
  

]

for i in range(100):
    regression.append(["Decision Tree Regression %.1f"%i, DecisionTreeRegressor(max_depth=i+1)]),
    regression.append(["Random Forrest Regressor %.1f"%i, RandomForestRegressor(max_depth=i+1, random_state=0,n_estimators=100)])
    regression.append(["Mult Layer Perceptron",MLPRegressor(solver='lbfgs', learning_rate ="adaptive",alpha=1e-10,hidden_layer_sizes=(i+1), random_state=1)])

model = 0
bestYpred = []
regFunction = []

maior = regression[0][1].fit(xTrain,yTrain).score(xTest,yTest)

for i,reg in enumerate(regression):
    score = 0
    reg[1].fit(xTrain,yTrain)
    yPred = reg[1].predict(xTest)
    score = reg[1].score(xTest,yTest)
    if score >= maior:
        maior = score
        score2 = reg[1].score(xTrain,yTrain)
        #if score2 >= maior2:
        regFunction = reg
        model = reg[0]   
        bestYpred = yPred
        maior2 = score2
       
        
    #print(reg[0]+" Score: %.2f" % score)
    #print("Mean squared error: %.2f" % mean_squared_error(yTest, yPred))
    print("**************")


regression = regFunction[1]


regression.fit(xTrain,yTrain)
yPred = regression.predict(xTest)


print("Melhor: ",regFunction[0])
print("Score: ",maior)
print("Mean squared error: %.5f" % mean_squared_error(yTest, yPred))
print("Explained Variance Score	 : %.2f" % metrics.explained_variance_score(yTest,yPred))    
print("Max Error	 : %.2f" % metrics.max_error(yTest,yPred))    
print("Mean Absolute Error	 : %.2f" % metrics.mean_absolute_error(yTest,yPred))    
print("Mean Squared Error	 : %.2f" % metrics.mean_squared_error(yTest,yPred))    




#print("Mean squared error: %.2f"% mean_squared_error(yTest, yPred))
# Explained variance score: 1 is perfect prediction
#print('Variance score: %.2f' % r2_score(yTest, yPred))
# The coefficients
# The mean squared error

#for i,prediction in enumerate(yPred):
#    print("Predicted : %s, Target: %s" %(prediction,yTest[i]))


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot outputs

ax.scatter(xTrain[:,0],xTrain[:,1], yTrain,  c=yTrain,cmap="coolwarm")

ax.set_xlabel('Latitude')
ax.set_ylabel('Longitude')
ax.set_zlabel('Correto')


fig4 = plt.figure()

ax4 = fig4.add_subplot(111, projection='3d')

p = regression.predict(xTrain) 

ax4.scatter(xTrain[:,0],xTrain[:,1], p,  c=p,cmap="coolwarm")

ax4.set_xlabel('Latitude')
ax4.set_ylabel('Longitude')
ax4.set_zlabel('predito')


fig2 = plt.figure()


ax2 = fig2.add_subplot(111)
ax2.scatter(xTest[:,0], yPred,c=yPred,cmap="coolwarm")
ax2.set_xlabel('Latitude')
ax2.set_ylabel('Predito')
#ax2.set_zlabel('Predito')

fig3 = plt.figure()
ax3 = fig3.add_subplot(111)

ax3.scatter(xTest[:,0], yTest,c=yTest,cmap="coolwarm")
ax3.set_xlabel('Latitude')
ax3.set_ylabel('Correto')
#ax3.set_zlabel('Correto')


plt.show()
